from langchain_ollama import OllamaEmbeddings
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma 
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda, RunnablePassthrough

load_dotenv()

embedding_llm = HuggingFaceEmbeddings(
    model_name= "sentence-transformers/all-MiniLM-L6-v2"
)

simple_llm  = ChatGroq(
     model="llama-3.1-8b-instant"
)

# embedding_llm = OllamaEmbeddings(
#     model="llama3"
# )

loading_video_ID = input("Enter the https://www.youtube.com/watch?v=---ID---- here(ONLY ID) : ")   # only the id of the video

transcript_extraction = YouTubeTranscriptApi.get_transcript(loading_video_ID, languages=["en"])

extracted_transcripts = " ".join(chunking_transcripts["text"] for chunking_transcripts in transcript_extraction)

splitting_document = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 80 )

document_split = splitting_document.create_documents(extracted_transcripts)

storing_vectore_docs = Chroma.from_documents(
    documents=document_split,
    embedding=embedding_llm,
    collection_name="my_collection"
    )

# Enable MMR in retriever for advance RAG
retriever_object = storing_vectore_docs.as_retriever(search_type = "mmr",search_kwargs = {"k":4, "lambda_mult":1})

retriever_prompt = PromptTemplate(
    template="""
        You are a expert assistant.
        Answer ONLY from the provided transcript context.
        If the context is insufficient, just say you don't know.
        {context}
        Question: {question}
""",
input_variables=["context","question"]
)

string_parser = StrOutputParser()

query = input("Enter the query : ")

retriever_output  = retriever_object.invoke(query)

def extracting_docs(retriever_output):
    context_text = "\n\n".join(extracted_document.page_content for extracted_document in retriever_output)
    return context_text

retriever_parallel_chain = RunnableParallel({
    'context': retriever_object | RunnableLambda(extracting_docs),
    'question': RunnablePassthrough()
})

final_retriever_chain = retriever_parallel_chain | retriever_prompt | simple_llm |string_parser


llm_output = final_retriever_chain.invoke(query)

print(llm_output)
